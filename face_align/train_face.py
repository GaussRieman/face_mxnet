import mxnet as mx
import mxnet.gluon.nn as nn
import d2lzh as d2l
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import data as gdata, loss as gloss, model_zoo
from mxnet.gluon import utils as gutils
import os
import time
import numpy as np
import random
import cv2

def get_batch_index(range, batch_size):
    batch = []
    slice = []
    random.seed(10)
    list = np.arange(range).tolist()
    while len(list)>batch_size:
        slice = random.sample(list, batch_size)
        list = [x for x in list if x not in slice]
        batch.append(slice)
    return batch

def get_batch_data(dir, batch):
    path = os.path.join(dir, 'list.txt')
    f = open(path)
    lines = f.readlines()
    length = len(lines)
    Xs , Ys = [], []
    transformer = gdata.vision.transforms.ToTensor()
    for i in range(len(batch)):
        line = lines[batch[i]].strip().split()
        img_name = line[0]
        X = cv2.imread(img_name)
        Y = np.asarray(list(map(float, line[1:201])), dtype=np.float).reshape(-1)
        # print(X)
        # print(Y)
        Xs.append(X)
        Ys.append(Y)

    np_xs = np.array(Xs)
    np_ys = np.array(Ys)
    mx_xs = mx.nd.array(np_xs)
    mx_ys = mx.nd.array(np_ys)
    return train_augs(mx_xs), mx_ys


def train(train_dir, batch, net, loss, trainer, batch_size, epoches):
    for epoch in range(epoches):
        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()
        for bat in batch:
            Xs, ys = get_batch_data(train_dir, bat)
            ls = []
            with autograd.record():
                y_hats = net(Xs)
                ls = [loss(y_hats, ys)]
            for l in ls:
                l.backward()
            trainer.step(batch_size)
            train_l_sum += sum([l.sum().asscalar() for l in ls])
            n += sum([l.size for l in ls])
        # test_acc = d2l.utils.evaluate_accuracy(test_iter, net, ctx)
        print('epoch %d, loss %.4f, n = %d, train acc %.3f, test acc %.3f, '
              'time %.1f sec'
              % (epoch + 1, train_l_sum / n, n, train_acc_sum / 1.0, 1.0,
                 time.time() - start))




def train_fine_tuning(train_dir, net, learning_rate, batch_size=32, num_epochs=10, length = 1448):
    train_batch_idx = get_batch_index(length, batch_size)
    # ctx = d2l.try_all_gpus()
    ctx = mx.cpu()
    net.collect_params().reset_ctx(ctx)
    net.hybridize()
    loss = gloss.L2Loss()
    trainer = gluon.Trainer(net.collect_params(), 'sgd', {
        'learning_rate': learning_rate, 'wd': 0.5})
    train(train_dir, train_batch_idx, net, loss, trainer, batch_size, num_epochs)


# 指定RGB三个通道的均值和方差来将图像通道归一化
normalize = gdata.vision.transforms.Normalize(
    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

train_augs = gdata.vision.transforms.Compose([
    # gdata.vision.transforms.RandomResizedCrop(224),
    # gdata.vision.transforms.RandomFlipLeftRight(),
    gdata.vision.transforms.ToTensor(),
    normalize])

test_augs = gdata.vision.transforms.Compose([
    gdata.vision.transforms.Resize(256),
    gdata.vision.transforms.CenterCrop(224),
    gdata.vision.transforms.ToTensor(),
    normalize])



# pretrained_net = model_zoo.vision.mobilenet_v2_0_25(pretrained=True)
#
# finetune_net = model_zoo.vision.mobilenet_v2_0_25(classes=200)
# finetune_net.features = pretrained_net.features
# finetune_net.output.initialize(init.Normal(sigma=0.05))
# # output中的模型参数将在迭代中使用10倍大的学习率
# finetune_net.output.collect_params().setattr('lr_mult', 10)

finetune_net = model_zoo.vision.mobilenet_v2_0_25(classes=200, pretrained  = False)

# TRANIN
finetune_net.initialize()
train_fine_tuning('train_data', finetune_net, 0.1)
finetune_net.save_parameters('facenet.params')




# TEST
# finetune_net.load_parameters('facenet.params')
# transformer = gdata.vision.transforms.ToTensor()
# train_file = 'train_data/imgs/0_51_Dresses_wearingdress_51_377_0.png'
#
# test_file = 'test_data/imgs/4_29_Students_Schoolkids_Students_Schoolkids_29_148_0.png'
# test_list = 'test_data/list.txt'
#
# f = open(test_list)
# labels = f.readlines()
# # label = '220 158 402 388 201.0749969482422 245.41200256347656 203.64097595214844 258.647705078125 206.55320739746094 271.81072998046875 210.16746520996094 284.7967834472656 214.8535919189453 297.4324035644531 220.87599182128906 309.48675537109375 228.1277618408203 320.8465576171875 236.33021545410156 331.54229736328125 245.25706481933594 341.6429748535156 254.7255401611328 351.239013671875 264.5885925292969 360.4294738769531 274.92572021484375 369.0804138183594 286.0174560546875 376.73175048828125 298.012451171875 382.8616027832031 310.7995910644531 387.08856201171875 324.1102294921875 389.1228942871094 337.5618896484375 388.6733703613281 348.27471923828125 385.8037109375 357.6256103515625 379.88983154296875 365.13165283203125 371.699462890625 372.28265380859375 363.18536376953125 379.80218505859375 354.9936828613281 386.4156494140625 346.06817626953125 391.3282470703125 336.1097106933594 394.4663391113281 325.4519958496094 396.4418029785156 314.5105895996094 398.14276123046875 303.5210876464844 399.7221374511719 292.51385498046875 400.74481201171875 281.44293212890625 400.92083740234375 270.32684326171875 400.25506591796875 259.228759765625 398.87774658203125 248.19554138183594 397.0264587402344 237.23062133789062 250.13002014160156 223.32200622558594 265.10101318359375 208.1690216064453 281.64801025390625 203.4300079345703 297.6050109863281 205.5469970703125 313.4630126953125 207.90199279785156 314.19500732421875 215.70199584960938 297.6920166015625 214.79501342773438 282.1900329589844 216.45501708984375 265.6400146484375 219.0120086669922 346.9540100097656 207.30099487304688 358.9129943847656 204.7570037841797 372.0420227050781 201.0370330810547 383.7040100097656 203.36302185058594 395.342041015625 217.41201782226562 384.2080383300781 212.86502075195312 372.15802001953125 211.37899780273438 359.37701416015625 213.01901245117188 347.447998046875 214.6800079345703 333.8609924316406 240.39100646972656 336.0619201660156 254.6343231201172 338.5596618652344 268.82305908203125 338.8642883300781 283.1104431152344 315.7330017089844 295.6400146484375 326.9175720214844 297.473876953125 338.2295837402344 297.9422607421875 343.7730712890625 297.5479736328125 349.30084228515625 296.9690246582031 262.38299560546875 244.5449981689453 272.3388366699219 237.8785400390625 283.9889831542969 235.33641052246094 296.36199951171875 237.82427978515625 305.69464111328125 246.39523315429688 295.24041748046875 248.3640899658203 284.6165466308594 248.89413452148438 273.1632385253906 248.41331481933594 350.87799072265625 245.6820068359375 357.9652404785156 236.78895568847656 369.2919616699219 234.88587951660156 378.8298034667969 236.50535583496094 386.2901306152344 242.76199340820312 378.74041748046875 246.64134216308594 370.4893798828125 248.540771484375 360.5535888671875 248.0796356201172 283.11749267578125 325.4329833984375 305.0022277832031 317.6381530761719 327.32342529296875 311.5201416015625 337.00482177734375 311.59295654296875 344.3435363769531 312.5697021484375 358.7122802734375 318.19140625 369.861572265625 328.9331359863281 361.8160400390625 339.6114196777344 350.41766357421875 346.4107666015625 337.2351379394531 348.4698181152344 316.85430908203125 347.1642761230469 298.54449462890625 338.8305358886719 286.8330078125 326.05902099609375 311.43408203125 322.2508544921875 336.2779541015625 320.576904296875 352.8221435546875 322.5228576660156 368.0701904296875 329.3791809082031 352.7513427734375 334.9115295410156 336.5931091308594 336.69879150390625 310.5755920410156 336.21441650390625 283.0664978027344 242.53021240234375 365.1780090332031 241.89614868164062'
# label = '66 67 152 190 67.66090393066406 117.76821899414062 69.53128814697266 122.52642822265625 70.49694061279297 127.53439331054688 69.90896606445312 132.59527587890625 68.85490417480469 137.59718322753906 68.5362548828125 142.697509765625 69.04803466796875 147.77833557128906 70.55261993408203 152.6605682373047 72.59876251220703 157.3471221923828 74.83161163330078 161.94866943359375 77.17996215820312 166.4922332763672 79.60688018798828 170.99453735351562 81.88978576660156 175.57032775878906 83.87628936767578 180.2833709716797 86.17074584960938 184.84657287597656 89.71627044677734 188.4866180419922 94.40361785888672 190.443115234375 101.42681884765625 190.4097137451172 108.26130676269531 188.6518096923828 114.85700225830078 186.129638671875 121.35530853271484 183.362060546875 127.72846221923828 180.3207550048828 133.72531127929688 176.60084533691406 138.95492553710938 171.8721923828125 143.1521759033203 166.20382690429688 146.29693603515625 159.88832092285156 148.4688262939453 153.17233276367188 149.9759063720703 146.273193359375 151.1803741455078 139.3135528564453 152.25059509277344 132.33203125 153.11537170410156 125.32227325439453 153.77386474609375 118.28996276855469 154.32240295410156 111.24808502197266 67.19343566894531 111.31880187988281 69.66224670410156 106.40066528320312 72.49352264404297 107.98002624511719 75.66191101074219 109.56683349609375 79.23956298828125 111.71036529541016 79.25421142578125 114.87108612060547 75.6590576171875 113.52774810791016 72.31196594238281 112.40052795410156 69.59400939941406 111.41374969482422 93.89930725097656 110.41471099853516 101.65531158447266 107.8365249633789 110.05858612060547 105.82228088378906 118.18909454345703 106.22127532958984 127.76640319824219 110.83055877685547 118.12126922607422 109.29866790771484 109.97152709960938 109.77900695800781 101.77584075927734 112.34758758544922 93.91724395751953 114.59371185302734 88.1437759399414 123.13174438476562 86.91329193115234 131.54249572753906 85.9915542602539 139.95396423339844 83.55866241455078 147.11033630371094 80.04193878173828 150.33921813964844 84.20759582519531 151.6233367919922 88.54531860351562 151.56954956054688 93.61880493164062 150.39651489257812 98.76679229736328 149.63462829589844 70.41693878173828 124.88831329345703 70.83051300048828 121.766357421875 73.19173431396484 119.92184448242188 78.83480072021484 120.646484375 83.11616516113281 124.43173217773438 78.40088653564453 125.90775299072266 73.4061508178711 126.4493637084961 71.81391143798828 125.87019348144531 100.59152221679688 123.99800109863281 103.45970916748047 120.7760009765625 107.45740509033203 119.2740478515625 112.98609161376953 120.26860809326172 118.31242370605469 122.20561218261719 113.4570541381836 124.68360900878906 108.14612579345703 125.64293670654297 104.3447265625 124.93934631347656 81.21563720703125 162.7611541748047 82.1851577758789 159.1667022705078 86.03545379638672 159.0203094482422 89.1314697265625 159.20848083496094 92.99732208251953 158.65264892578125 102.97270965576172 159.26991271972656 112.99374389648438 159.4341278076172 105.60233306884766 164.4689483642578 97.44075012207031 168.0513153076172 88.71666717529297 169.75364685058594 85.1062240600586 168.86367797851562 82.50657653808594 166.2677459716797 82.04786682128906 162.85018920898438 85.60679626464844 163.2826690673828 89.18599700927734 163.45846557617188 99.59762573242188 162.3616943359375 109.91494750976562 160.57177734375 99.58368682861328 162.36349487304688 89.15787506103516 163.45880126953125 85.5927963256836 163.28134155273438 75.32048797607422 122.91885375976562 108.27320098876953 121.7419662475586'
#
# l = label.strip().split()
# l = [float(x) for x in l]
# print(l)
#
# img = cv2.imread(train_file)
# img = cv2.resize(img, (224,224))
# t_img = transformer(mx.nd.array(img))
# t_img = mx.ndarray.expand_dims(t_img, axis = 0)
# out = finetune_net(t_img)
# out_label = out[0]
# print(out.shape)
# print(out_label.shape)
#
# cv2.rectangle(img, (66, 67), (152, 190), (255,0,0), 1)
# cv2.rectangle(img, (77, 39), (130, 120), (0,255,0), 1)
# # for i in range(98):
# #     cv2.circle(img, out_label[])
# cv2.imshow('img', img)
# cv2.waitKey(0)
# print(out)















